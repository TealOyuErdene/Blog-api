[{"id":"86fbb638-c056-4a6a-b2c4-894542e73a60","title":"GPT Is an Unreliable Information Store","text":"<p>Large language models (or generative pre-trained transformers, GPT) need more reliable information accuracy checks to be considered for Search.</p><p>These models are great at creative applications such as storytelling, art, or music and creating privacy-preserving synthetic data for applications.<br>These models fail, however, at consistent factual accuracy due to <a href=\"https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/\">AI hallucinations</a> and transfer learning limitations in ChatGPT, Bing Chat, and Google Bard.</p><p>First, let’s define what AI hallucinations are. There are instances where a large language model creates information that is not based on factual evidence but may be influenced by its transformer architecture’s bias or erroneous decoding. In other words, the model makes up facts, which can be problematic in domains where factual accuracy is critical.</p><p>Ignoring consistent factual accuracy is dangerous in a world where accurate and reliable information is paramount in battling misinformation and disinformation.</p><p>Search companies should reconsider “re-inventing search” by mixing Search with unfiltered GPT-powered chat modalities to avoid potential harm to public health, political stability, or social cohesion.</p><p>&nbsp;</p><p>This article extends this assertion with an example of how ChatGPT is convinced that I have been dead for four years and how my obituary, which seems very real, highlights the risks of using GPTs for search-based information retrieval. You can try it by plugging my name into ChatGPT and then convince it that I’m alive.</p><p>A few weeks ago, I decided to dive into some light research after learning that Google wiped $100 billion off its market cap because of a rushed demo where Bard, the ChatGPT competitor, shared some inaccurate information. The market seems to <a href=\"https://www.cnbc.com/video/2023/02/16/the-new-york-times-kevin-roose-on-his-conversation-with-microsofts-ai-powered-chatbot-bing.html\">react negatively to the reliability and trustworthiness of this tech</a>, but I don’t feel we’re connecting these concerns with the medium enough.</p><p>I decided to “egosurf” on ChatGPT. <i>Note: I just discovered the word egosurf</i>. We’ve all Googled ourselves before but this time with ChatGPT.<br>This decision was intentional because what better way to test for factual accuracy than to ask it about me? And this decision didn’t disappoint; I consistently got the same result: <strong>I learned I was dead.</strong></p>","categoryId":"ba816c8a-f3b4-4d74-a92e-d58fe967df2e","image":"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9Msl9UY6vp_c-dIHzYqPQ.jpeg"}]